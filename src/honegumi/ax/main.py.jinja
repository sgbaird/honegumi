# Generated by Honegumi (https://arxiv.org/abs/2502.06815)
# %pip install ax-platform==0.4.3 {% if visualize %}matplotlib{% endif %}
{%- if existing_data or visualize %}
import numpy as np
import pandas as pd
from ax.service.ax_client import AxClient, ObjectiveProperties
{% if visualize %}import matplotlib.pyplot as plt{% endif %}
{% else %}
import numpy as np
from ax.service.ax_client import AxClient, ObjectiveProperties
{% endif %}

{# List of expected models #}
{% set expected_models = ["Default", "Custom", "Fully Bayesian"] %}

{# Check if model is in the list of expected models #}
{% if model not in expected_models %}
    {{ 0/0 }}
{% endif %}

{% if custom_gen -%}
from ax.modelbridge.factory import Models
from ax.modelbridge.generation_strategy import GenerationStep, GenerationStrategy
{%- endif %}
{% if model == "Custom" -%}
from botorch.acquisition import UpperConfidenceBound
{%- endif %}

{% if task == "Multi" -%}
from ax.modelbridge.registry import Specified_Task_ST_MTGP_trans
from ax.core.observation import ObservationFeatures
{%- endif %}

obj1_name = "branin"
{% if objective == "Multi" -%}
obj2_name = "branin_swapped"
{%- endif %}

# Define the objective function(s) to optimize
# This function evaluates candidate parameter sets and returns the objective value(s)
# In a real application, replace this with your actual experiment or simulation
def branin{% if composition_constraint %}3{% endif %}{% if objective == "Multi" %}_moo{% endif %}{% if task == "Multi" %}_mt{% endif %}(x1, x2{% if composition_constraint %}, x3{% endif %}{% if categorical %}, c1{% endif %}{% if task == "Multi" %}, task{% endif %}):
    y = float(
        (x2 - 5.1 / (4 * np.pi**2) * x1**2 + 5.0 / np.pi * x1 - 6.0) ** 2
        + 10 * (1 - 1.0 / (8 * np.pi)) * np.cos(x1)
        + 10
    )

    {% if task == "Multi" %}
    # if multi-task, add a penalty based on task
    penalty_lookup = {"A": 1.0, "B": 1.1 + x1 + 2*x2}
    y += penalty_lookup[task]
    {% endif %}

    {% if composition_constraint -%}
    # Contrived way to incorporate x3 into the objective
    y = y * (1 + 0.1 * x1 * x2 * x3)
    {%- endif %}

    {% if categorical %}
    # add a made-up penalty based on category
    penalty_lookup = {"A": 1.0, "B": 0.0, "C": 2.0}
    y += penalty_lookup[c1]
    {% endif %}
    {% if objective == "Multi" -%}
    # second objective has x1 and x2 swapped
    y2 = float(
        (x1 - 5.1 / (4 * np.pi**2) * x2**2 + 5.0 / np.pi * x2 - 6.0) ** 2
        + 10 * (1 - 1.0 / (8 * np.pi)) * np.cos(x2)
        + 10
    )
    {% if task == "Multi" %}
    # if multi-task, add a penalty based on task for second objective
    penalty_lookup_2 = {"A": 0.8, "B": 0.9 + 2*x1 + x2}
    y2 += penalty_lookup_2[task]
    {% endif %}

    {% if composition_constraint -%}
    # Contrived way to incorporate x3 into the second objective
    y2 = y2 * (1 - 0.1 * x1 * x2 * x3)
    {%- endif %}
    {% if categorical %}
    # add a made-up penalty based on category
    penalty_lookup = {"A": 0.0, "B": 2.0, "C": 1.0}
    y2 += penalty_lookup[c1]
    {% endif %}
    return {obj1_name: y, obj2_name: y2}
    {% else %} {# single objective #}
    return y
    {%- endif %}

{% set objective_function = "branin" %}

{% if composition_constraint -%}
    {% set objective_function = objective_function + "3" %}
{% endif %}

{# Set the objective function based on the objective #}
{% if objective == "Multi" -%}
    {% set objective_function = objective_function + "_moo" %}
{%- endif %}

{% if task == "Multi" %}
    {% set objective_function = objective_function + "_mt" %}
{% endif %}

{# add MOO to FULLYBAYESIAN if multi-objective #}
{% if model == "Fully Bayesian" and objective == "Multi" -%}
    {% set model_name = "SAASBO" %}
{% elif model == "Fully Bayesian" and objective != "Multi" -%}
    {% set model_name = "SAASBO" %} {# apparently this handles both soo and moo, https://ax.dev/docs/tutorials/saasbo_nehvi/ #}
{% else %}
    {% set model_name = "BOTORCH_MODULAR" %}
{%- endif %}

{% if composition_constraint %}
# Define total for compositional constraint, where x1 + x2 + x3 == total
total = 10.0
{% endif %}

{% if existing_data -%} {# categorical inline/repeated because will be clearer to readers #}
# Define the training data to initialize the optimizer
# Existing data helps the optimizer learn about the design space before suggesting new trials
# Each row represents one previously evaluated parameter set
{% if composition_constraint %}
# note that for this training data, the compositional constraint is satisfied
{% endif %}
{% if order_constraint %}
# note that for this training data, the order constraint is satisfied
{% endif %} {# TODO: REVIEW: consider how to incorporate out of design with the above scheme. Probably add an additional point that is for sure out of design and warn user if there are points like this? #}
{# Set some shorthand variables to make X_train more readable #}
{% set mt = task == "Multi"%}
{% set comp = composition_constraint %}
{% set cat = categorical %}
{% set ord = order_constraint %}
{# Note the lack of comma after x2, and the presence of a comma before the x3 #}
X_train = pd.DataFrame([
    {
        "x1": {% if comp %}4.0{% else %}-3.0{% endif %},
        "x2": 5.0
        {% if mt %}, "task": "A"{% endif %}
        {% if comp %},"x3": 1.0{% endif %}
        {% if cat %},"c1": "A"{% endif %}
    },
    {
        "x1": 0.0,
        "x2": 6.2
        {% if mt %}, "task": "B"{% endif %}
        {% if comp %},"x3": 3.8{% endif %}
        {% if cat %},"c1": "B"{% endif %}
    },
    {
        "x1": {% if ord %}2.1{% else %}5.9{% endif %},
        "x2": {% if ord %}5.9{% else %}2.0{% endif %}
        {% if mt %}, "task": "A"{% endif %}
        {% if comp %},"x3": 2.0{% endif %}
        {% if cat %},"c1": "C"{% endif %}
    },
    {
        "x1": 1.5,
        "x2": 2.0
        {% if mt %}, "task": "B"{% endif %}
        {% if comp %},"x3": 6.5{% endif %}
        {% if cat %},"c1": "A"{% endif %}
    },
    {
        "x1": 1.0,
        "x2": 9.0
        {% if mt %}, "task": "A"{% endif %}
        {% if comp %},"x3": 0.0{% endif %}
        {% if cat %},"c1": "B"{% endif %}
    }
])

# Define y_train (normally the values would be supplied directly instead of calculating here)
y_train = [
    {{ objective_function }}(
        row["x1"],
        row["x2"]
        {% if composition_constraint %}, row["x3"]{% endif %}
        {% if categorical %}, row["c1"]{% endif %}
        {% if task == "Multi" %}, row["task"]{% endif %}
    )
    for _, row in X_train.iterrows()
]

# See https://youtu.be/4tnaL9ts6CQ for simple human-in-the-loop BO instructions

# Define the number of training examples
n_train = len(X_train)
{%- endif %}

{# Logic around the number of iterations #}
{# A reasonable default for initialization points is 2 * number of parameters #}
{% set num_init = 4 + 2 * (categorical + composition_constraint) if not dummy else 3 %}
{% set num_bayes = 15 if not dummy else 2 %}
{% set num_iter = num_init + num_bayes %}

{% if custom_gen -%}
# Define a custom generation strategy with explicit initialization and optimization phases
# GenerationStep controls how trials are suggested at different stages of optimization
gs = GenerationStrategy(
    steps=[
        GenerationStep(
            model=Models.SOBOL,
            num_trials={{ num_init }}, # how many sobol trials to perform (rule of thumb: 2 * number of params)
            min_trials_observed=3, # minimum trials that must complete before moving to next step
            max_parallelism=5, # maximum number of trials to run simultaneously
            {% if task == "Multi" %}
            model_kwargs={"seed": 999, "transforms": Specified_Task_ST_MTGP_trans},
            model_gen_kwargs={"deduplicate": True},
            {% else %}
            model_kwargs={"seed": 999},
            {% endif %}
        ),
        GenerationStep(
            model=Models.{{ model_name }}, # Bayesian optimization model for adaptive sampling
            num_trials=-1, # -1 means use this step for all remaining trials
            max_parallelism=3, # maximum number of trials to run simultaneously
            model_kwargs={
                {% if task == "Multi" %}
                    {% if model == "Fully Bayesian" %}
                        {# "num_samples": 512, #} {# https://github.com/sgbaird/honegumi/issues/87 #}
                        {# "warmup_steps": 512, #}
                        "transforms": Specified_Task_ST_MTGP_trans
                    {% elif model == "Custom" %}
                        "botorch_acqf_class": UpperConfidenceBound,
                        "transforms": Specified_Task_ST_MTGP_trans
                    {% else %}
                        "transforms": Specified_Task_ST_MTGP_trans
                    {% endif %}
                {% elif model == "Fully Bayesian" %}
                    {# "num_samples": 512, #} {# https://github.com/sgbaird/honegumi/issues/87 #}
                    {# "warmup_steps": 512 #}
                {% elif model == "Custom" %}
                    "botorch_acqf_class": UpperConfidenceBound
                {% else %}
                    {}
                {% endif %}
            },
        ),
    ]
)
{%- endif %}

# Initialize the Ax client, which manages the optimization experiment
# AxClient provides a high-level interface for Bayesian optimization
ax_client = AxClient({% if custom_gen %}generation_strategy=gs{% endif %})
{% if composition_constraint -%}
{%- endif %}
# Create the experiment with parameter space definition and optimization objectives
# Parameters define the search space; objectives define what to optimize
ax_client.create_experiment(
    parameters=[
        {"name": "x1", "type": "range", "bounds": {% if composition_constraint %}[0.0, total]{% else %}[-5.0, 10.0]{% endif %}},
        {"name": "x2", "type": "range", "bounds": {% if composition_constraint %}[0.0, total]{% else %}[0.0, 10.0]{% endif %}},
        {% if task == "Multi"%}
            {
                "name": "task",
                "type": "choice",
                "values": ["A", "B"],
                "is_task": True,
                "target_value": "B"
            }{% if categorical %},{% endif %}
        {% endif %}
        {% if categorical %}
            {
                "name": "c1",
                "type": "choice",
                "is_ordered": False,
                "values": ["A", "B", "C"]
            }
        {% endif %}
    ],
    objectives={
        obj1_name: ObjectiveProperties(minimize=True{% if custom_threshold %}, threshold=25.0{% endif %}),
{% if objective == "Multi" -%}
        obj2_name: ObjectiveProperties(minimize=True{% if custom_threshold %}, threshold=15.0{% endif %}),
{%- endif %}
    },
{% if sum_constraint or composition_constraint or order_constraint or linear_constraint %}
    parameter_constraints=[
        {% if sum_constraint %}
            {% if composition_constraint %}
                "x1 + x2 <= 15.0", # example of a sum constraint, which may be redundant/unintended if composition_constraint is also selected
            {% else %}
                "x1 + x2 <= 15.0", # example of a sum constraint
            {% endif %}
        {% endif %}
        {% if composition_constraint %}f"x1 + x2 <= {total}", # reparameterized compositional constraint, which is a type of sum constraint
        {% endif %}
        {% if order_constraint %}"x1 <= x2", # example of an order constraint
        {% endif %}
        {% if linear_constraint %}"1.0*x1 + 0.5*x2 <= 15.0", # example of a linear constraint. Note the lack of space around the asterisks
        {% endif %}
        ],
{% endif %}
)

{% if existing_data -%}
# Add existing data to the AxClient
# This allows the optimizer to learn from previous experiments before suggesting new trials
for i in range(n_train):
    parameterization = X_train.iloc[i].to_dict()

    {% if composition_constraint %}
    # remove x3, since it's hidden from search space due to composition constraint
    parameterization.pop('x3')
    {% endif %}

    ax_client.attach_trial(parameterization)
    ax_client.complete_trial(trial_index=i, raw_data=y_train[i]) # raw_data is the observed objective value(s)
{%- endif %}
{# One BO step if dummy, TODO: change to 2+ if batch or asynchronous is selected #}
{% if synchrony == "Batch" %}
batch_size = 2
{% endif %}
{% set indent = "    " if synchrony == "Batch" else "" %}

# Main optimization loop: request new trials, evaluate them, and report results
# The optimizer learns from each completed trial to suggest better candidates
for i in range({{ num_iter }}):
    {% if synchrony == "Single" and not task == "Multi" %}
    parameterization, trial_index = ax_client.get_next_trial()
    {% elif synchrony == "Single" and task == "Multi" %}
    parameterization, trial_index = ax_client.get_next_trial(fixed_features=ObservationFeatures({"task": "A" if i % 2 == 0 else "B"}))
    {% elif synchrony == "Batch" and task == "Multi" %}
    parameterizations, optimization_complete = ax_client.get_next_trials(batch_size, fixed_features=ObservationFeatures({"task": "A" if i % 2 == 0 else "B"}))
    for trial_index, parameterization in list(parameterizations.items()):
    {% else %} {# if batch and task = singlee #}
    parameterizations, optimization_complete = ax_client.get_next_trials(batch_size)
    for trial_index, parameterization in list(parameterizations.items()):
    {%- endif %}
    # extract parameters {# Consider using **parameters instead, but might require explanation and a link to ** docs #}
    {{ indent }}x1 = parameterization["x1"]
    {{ indent }}x2 = parameterization["x2"]
    {% if composition_constraint -%}
    {{ indent }}x3 = total - (x1 + x2) # composition constraint: x1 + x2 + x3 == total
    {%- endif %}
    {% if task == "Multi" %}
    {{ indent }}task = parameterization["task"]
    {%- endif %}
    {% if categorical -%}
    {{ indent }}c1 = parameterization["c1"]
    {%- endif %}

    {{ indent }}results = {{ objective_function }}(
    {{ indent }}    x1, x2{% if composition_constraint %}, x3{% endif %}{% if categorical %}, c1{% endif %}{% if task == "Multi" %}, task{% endif %}
    {{ indent }}    )
    {{ indent }}ax_client.complete_trial(trial_index=trial_index, raw_data=results) # Report results back to optimizer
{% if objective == "Multi" and task == "Single" -%}
# Extract Pareto optimal solutions (non-dominated points in multi-objective space)
pareto_results = ax_client.get_pareto_optimal_parameters()
{% elif objective == "Single" %}
# Extract the best parameters found during optimization
best_parameters, metrics = ax_client.get_best_parameters()
{%- endif %}

{% if visualize %}
# Plot results to visualize optimization progress
# These plots help assess convergence and identify potential issues
objectives = ax_client.objective_names
df = ax_client.get_trials_data_frame()
{%- if synchrony == "Batch" and objective == "Single"%}
df.index = df.index // batch_size # Adjust index for batch trials
{%- endif %}

fig, ax = plt.subplots(figsize=(6, 4), dpi=150)

{%- if objective == "Single" %}
{%- if task == "Multi" %}
task = "A"  # specify task results to plot
df = df[df.task == task]
ax.scatter(df.index, df[objectives], ec="k", fc="none", label="Observed")
ax.plot(df.index, np.minimum.accumulate(df[objectives]), color="#0033FF", lw=2, label="Best to Trial") # Track best objective value found so far
ax.set_title(f"Task {task}")
{%- else %}
ax.scatter(df.index, df[objectives], ec="k", fc="none", label="Observed")
ax.plot(df.index, np.minimum.accumulate(df[objectives]), color="#0033FF", lw=2, label="Best to Trial") # Track best objective value found so far
{%- endif %}
ax.set_xlabel("Trial Number")
ax.set_ylabel(objectives[0])
{%- elif task == "Multi" and objective == "Multi" %}
# The Pareto front represents the set of solutions in multi-objective optimization where no objective can be improved without worsening another. 
# Extracting the Pareto front helps identify the best trade-offs between competing objectives, which is crucial for understanding the optimal solutions in multi-objective problems.
def get_pareto(df, task='A'):
    t = df[df['task'] == task]
    m = t.iloc[:,4:6].values  # objectives are always cols 4,5
    p = ~np.any(np.all(m[:,None] > m, axis=2), axis=1)
    return [(dict(r[df.columns[6:-1]]), dict(r[df.columns[4:6]])) for i,r in t[p].iterrows()]

task = "A"
df = df[df.task == task]
pareto = get_pareto(df, task)
pareto_data = [p[1] for p in pareto]
pareto = pd.DataFrame(pareto_data).sort_values(objectives[0])

ax.scatter(df[objectives[0]], df[objectives[1]], fc="None", ec="k", label="Observed")
ax.plot(pareto[objectives[0]], pareto[objectives[1]], color="#0033FF", lw=2, label="Pareto Front")
ax.set_title(f"Task {task}")
ax.set_xlabel(objectives[0])
ax.set_ylabel(objectives[1])
{%- else %}
# Extract and plot Pareto front for multi-objective optimization
pareto = ax_client.get_pareto_optimal_parameters(use_model_predictions=False)
pareto_data = [p[1][0] for p in pareto.values()]
pareto = pd.DataFrame(pareto_data).sort_values(objectives[0])

ax.scatter(df[objectives[0]], df[objectives[1]], fc="None", ec="k", label="Observed")
ax.plot(pareto[objectives[0]], pareto[objectives[1]], color="#0033FF", lw=2, label="Pareto Front")
ax.set_xlabel(objectives[0])
ax.set_ylabel(objectives[1])
{%- endif %}

ax.legend()
plt.show()
{%- endif %}
