# Generated by Honegumi (https://arxiv.org/abs/2502.06815)
# %pip install ax-platform matplotlib
import matplotlib.pyplot as plt
import pandas as pd
from ax.modelbridge.factory import Models
from ax.modelbridge.generation_strategy import GenerationStep, GenerationStrategy
from ax.service.ax_client import AxClient, ObjectiveProperties

hardness_key = "hardness"
creep_ratio_key = "creep_ratio"


def branin3_moo(
    d1_929_59_9,
    d2_2579_20_6,
    d3_124_09_4,
    d4_107_15_3,
    n1_14970_87_7,
    n2_1191_43_1,
    n3_1191_67_9,
    n4_7575_23_7,
    c1,
):
    # Example: maximize hardness and creep_ratio (contrived functions)
    # Hardness increases with d1, d2, d4, n2, n3, penalized by d3, n1, n4, and c1
    hardness = (
        2.0 * d1_929_59_9
        + 1.5 * d2_2579_20_6
        + 1.0 * d4_107_15_3
        + 1.2 * n2_1191_43_1
        + 0.9 * n3_1191_67_9
        - 0.5 * d3_124_09_4
        - 0.7 * n1_14970_87_7
        - 0.6 * n4_7575_23_7
    )
    penalty_lookup = {"A": 0.0, "B": -1.0, "C": -2.0}
    hardness += penalty_lookup[c1]

    # Creep ratio increases with d3, n1, n4, and c1, penalized by d1, d4, n2
    creep_ratio = (
        1.2 * d3_124_09_4
        + 0.8 * n1_14970_87_7
        + 0.7 * n4_7575_23_7
        - 0.3 * d1_929_59_9
        - 0.2 * d4_107_15_3
        - 0.4 * n2_1191_43_1
    )
    penalty_lookup2 = {"A": 0.5, "B": 1.0, "C": 0.0}
    creep_ratio += penalty_lookup2[c1]

    return {
        hardness_key: hardness,
        creep_ratio_key: creep_ratio,
    }


# Define total for compositional constraint, where x1 + x2 + x3 == total
total = 1.0


# Define the training data

# note that for this training data, the compositional constraint is satisfied


# Example training data for 8 variables (d1-d4, n1-n4) that sum to total
X_train = pd.DataFrame(
    [
        {
            "d1_929_59_9": 0.1,
            "d2_2579_20_6": 0.1,
            "d3_124_09_4": 0.1,
            "d4_107_15_3": 0.1,
            "n1_14970_87_7": 0.1,
            "n2_1191_43_1": 0.1,
            "n3_1191_67_9": 0.2,
            "n4_7575_23_7": 0.2,
            "c1": "A",
        },
        {
            "d1_929_59_9": 0.0,
            "d2_2579_20_6": 0.2,
            "d3_124_09_4": 0.2,
            "d4_107_15_3": 0.1,
            "n1_14970_87_7": 0.1,
            "n2_1191_43_1": 0.1,
            "n3_1191_67_9": 0.2,
            "n4_7575_23_7": 0.1,
            "c1": "B",
        },
        {
            "d1_929_59_9": 0.1,
            "d2_2579_20_6": 0.1,
            "d3_124_09_4": 0.2,
            "d4_107_15_3": 0.1,
            "n1_14970_87_7": 0.1,
            "n2_1191_43_1": 0.1,
            "n3_1191_67_9": 0.1,
            "n4_7575_23_7": 0.2,
            "c1": "C",
        },
        {
            "d1_929_59_9": 0.2,
            "d2_2579_20_6": 0.1,
            "d3_124_09_4": 0.1,
            "d4_107_15_3": 0.1,
            "n1_14970_87_7": 0.1,
            "n2_1191_43_1": 0.1,
            "n3_1191_67_9": 0.1,
            "n4_7575_23_7": 0.2,
            "c1": "A",
        },
        {
            "d1_929_59_9": 0.1,
            "d2_2579_20_6": 0.2,
            "d3_124_09_4": 0.1,
            "d4_107_15_3": 0.1,
            "n1_14970_87_7": 0.1,
            "n2_1191_43_1": 0.1,
            "n3_1191_67_9": 0.1,
            "n4_7575_23_7": 0.2,
            "c1": "B",
        },
    ]
)

# Define y_train (normally the values would be supplied directly instead of
# calculating here)
y_train = [
    branin3_moo(
        row["d1_929_59_9"],
        row["d2_2579_20_6"],
        row["d3_124_09_4"],
        row["d4_107_15_3"],
        row["n1_14970_87_7"],
        row["n2_1191_43_1"],
        row["n3_1191_67_9"],
        row["n4_7575_23_7"],
        row["c1"],
    )
    for _, row in X_train.iterrows()
]

# See https://youtu.be/4tnaL9ts6CQ for simple human-in-the-loop BO instructions

# Define the number of training examples
n_train = len(X_train)


gs = GenerationStrategy(
    steps=[
        GenerationStep(
            model=Models.SOBOL,
            num_trials=8,  # how many sobol trials to perform (rule of thumb: 2 * number of params) # noqa: E501
            min_trials_observed=3,
            max_parallelism=5,
            model_kwargs={"seed": 999},
        ),
        GenerationStep(
            model=Models.SAASBO,
            num_trials=-1,
            max_parallelism=3,
            model_kwargs={},
        ),
    ]
)

ax_client = AxClient(generation_strategy=gs)

# NChooseK, no more than 2 from d at a given time, no more than 2 from n at a
# given time

# dispensing multiple materials increases the dispense time

# resolution of dispensing gets difficult with many materials (300 uL for 96
# well plate, 1 mL if 48 well plate)

# OT-Flex has 50, 200, and 1000 uL tips, but 50 uL is narrow / doesn't work well
# with viscous materials

# intention to make stock mixtures for replicates (final sample volumes at
# minimum ~50 uL)

# perhaps intervals of 0.01

# interpretability gets more difficult (e.g., isolating to effects from specific
# ingredients)

ax_client.create_experiment(
    parameters=[
        {"name": "d1_929_59_9", "type": "range", "bounds": [0.0, total]},
        {"name": "d2_2579_20_6", "type": "range", "bounds": [0.0, total]},
        {"name": "d4_107_15_3", "type": "range", "bounds": [0.0, total]},
        {"name": "n1_14970_87_7", "type": "range", "bounds": [0.0, total]},
        {"name": "n2_1191_43_1", "type": "range", "bounds": [0.0, total]},
        {"name": "n3_1191_67_9", "type": "range", "bounds": [0.0, total]},
        {"name": "n4_7575_23_7", "type": "range", "bounds": [0.0, total]},
        {
            "name": "c1",
            "type": "choice",
            "is_ordered": False,
            "values": ["A", "B", "C"],
        },
    ],
    objectives={
        hardness_key: ObjectiveProperties(
            minimize=False,
            threshold=1000000,
        ),  # Pascals, unreliable measurements if lower than 10 kPa., greater than 1 GPa # noqa: E501
        creep_ratio_key: ObjectiveProperties(
            minimize=False,
            threshold=0.5,
        ),  # 0 to 1
    },
    parameter_constraints=[
        (
            "d1_929_59_9 + d2_2579_20_6 + d4_107_15_3 + n1_14970_87_7 + "
            "n2_1191_43_1 + n3_1191_67_9 + n4_7575_23_7 <= {total}"
        ).format(
            total=total
        ),  # reparameterized compositional constraint
    ],
)

# Add existing data to the AxClient
for i in range(n_train):
    parameterization = X_train.iloc[i].to_dict()
    # remove d3_124_09_4, n2_1191_43_1, n3_1191_67_9, n4_7575_23_7, since
    # they're hidden from search space due to composition constraint
    for x_hidden in [
        "d3_124_09_4",
        "n2_1191_43_1",
        "n3_1191_67_9",
        "n4_7575_23_7",
    ]:
        parameterization.pop(x_hidden)
    ax_client.attach_trial(parameterization)
    ax_client.complete_trial(trial_index=i, raw_data=y_train[i])


batch_size = 2


for i in range(23):

    parameterizations, optimization_complete = ax_client.get_next_trials(batch_size)
    for trial_index, parameterization in list(parameterizations.items()):
        # extract parameters
        d1_929_59_9 = parameterization["d1_929_59_9"]
        d2_2579_20_6 = parameterization["d2_2579_20_6"]
        d4_107_15_3 = parameterization["d4_107_15_3"]
        n1_14970_87_7 = parameterization["n1_14970_87_7"]
        n2_1191_43_1 = parameterization["n2_1191_43_1"]
        n3_1191_67_9 = parameterization["n3_1191_67_9"]
        n4_7575_23_7 = parameterization["n4_7575_23_7"]
        # composition constraint: d1 + d2 + d3 + d4 + n1 + n2 + n3 + n4 == total
        d3_124_09_4 = total - (
            d1_929_59_9
            + d2_2579_20_6
            + d4_107_15_3
            + n1_14970_87_7
            + n2_1191_43_1
            + n3_1191_67_9
            + n4_7575_23_7
        )

        c1 = parameterization["c1"]

        results = branin3_moo(
            d1_929_59_9,
            d2_2579_20_6,
            d3_124_09_4,
            d4_107_15_3,
            n1_14970_87_7,
            n2_1191_43_1,
            n3_1191_67_9,
            n4_7575_23_7,
            c1,
        )
        ax_client.complete_trial(trial_index=trial_index, raw_data=results)
pareto_results = ax_client.get_pareto_optimal_parameters()


# Plot results
objectives = ax_client.objective_names
df = ax_client.get_trials_data_frame()

fig, ax = plt.subplots(figsize=(6, 4), dpi=150)
pareto = ax_client.get_pareto_optimal_parameters(use_model_predictions=False)
pareto_data = [p[1][0] for p in pareto.values()]
pareto = pd.DataFrame(pareto_data).sort_values(objectives[0])

ax.scatter(
    df[objectives[0]],
    df[objectives[1]],
    fc="None",
    ec="k",
    label="Observed",
)
ax.plot(
    pareto[objectives[0]],
    pareto[objectives[1]],
    color="#0033FF",
    lw=2,
    label="Pareto Front",
)
ax.set_xlabel("hardness")
ax.set_ylabel("creep_ratio")

ax.legend()
plt.show()
